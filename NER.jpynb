# Importing required libraries
from google.colab import files
import pandas as pd
import numpy as np
from itertools import chain
from sklearn.model_selection import train_test_split
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Attention, Input
from tensorflow.keras.callbacks import EarlyStopping
import spacy
from spacy import displacy

# Step 1: File upload and data loading
uploaded = files.upload()
data = pd.read_csv('ner_dataset.csv', encoding='unicode_escape')

# Step 2: Define functions for token and tag mappings
def get_dict_map(data, token_or_tag):
    if token_or_tag == 'token':
        vocab = list(set(data['Word'].to_list()))
    else:
        vocab = list(set(data['Tag'].to_list()))
    
    idx2tok = {idx: tok for idx, tok in enumerate(vocab)}
    tok2idx = {tok: idx for idx, tok in enumerate(vocab)}
    return tok2idx, idx2tok

# Create token and tag mappings
token2idx, idx2token = get_dict_map(data, 'token')
tag2idx, idx2tag = get_dict_map(data, 'tag')

# Map 'Word' and 'Tag' columns to their respective indices
data['Word_idx'] = data['Word'].map(token2idx)
data['Tag_idx'] = data['Tag'].map(tag2idx)

# Step 3: Handle missing values and group the data by sentences
data_fillna = data.fillna(method='ffill', axis=0)

# Group the data by sentences and aggregate the relevant columns
data_group = data_fillna.groupby(['Sentence #'], as_index=False).agg({
    'Word': list, 
    'POS': list, 
    'Tag': list, 
    'Word_idx': list, 
    'Tag_idx': list
})

# Step 4: Define a function to pad and split the data into train, test, and validation sets
def get_pad_train_test_val(data_group, data):
    # Get the number of unique tokens and tags
    n_token = len(list(set(data['Word'].to_list()))) + 1  # +1 for padding
    n_tag = len(list(set(data['Tag'].to_list())))

    # Pad tokens (X variable)
    tokens = data_group['Word_idx'].tolist()
    maxlen = max([len(s) for s in tokens])
    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value=n_token - 1)

    # Pad tags (y variable) and convert to one-hot encoding
    tags = data_group['Tag_idx'].tolist()
    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value=tag2idx["O"])
    n_tags = len(tag2idx)
    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]

    # Split the dataset into train, test, and validation sets
    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, random_state=2020)
    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_, tags_, test_size=0.25, random_state=2020)

    print(
        'train_tokens length:', len(train_tokens),
        '\ntest_tokens length:', len(test_tokens),
        '\nval_tokens length:', len(val_tokens)
    )

    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags

# Call the function to split the data
train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)

# Step 5: Define the Bi-LSTM with Attention model
def get_bilstm_attention_model(input_dim, output_dim, input_length, n_tags):
    inputs = Input(shape=(input_length,))
    embedding = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length)(inputs)
    
    # Bidirectional LSTM
    lstm_out = Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(embedding)
    
    # Attention Layer
    attention = Attention()([lstm_out, lstm_out])
    
    # Output Layer
    dense_out = TimeDistributed(Dense(n_tags, activation="softmax"))(attention)
    
    # Compile model
    model = Sequential(inputs=inputs, outputs=dense_out)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.summary()
    
    return model

# Define input dimensions
input_dim = len(list(set(data['Word'].to_list()))) + 1  # +1 for padding
output_dim = 64
input_length = max([len(s) for s in data_group['Word_idx'].tolist()])
n_tags = len(tag2idx)

# Instantiate the model
model_bilstm_attention = get_bilstm_attention_model(input_dim, output_dim, input_length, n_tags)

# Step 6: Training the model with early stopping
def train_model(X, y, model):
    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    hist = model.fit(X, y, batch_size=32, verbose=1, epochs=25, validation_split=0.2, callbacks=[early_stopping])
    return hist.history

# Train the model
results = pd.DataFrame()
results['with_attention'] = train_model(train_tokens, np.array(train_tags), model_bilstm_attention)

# Step 7: Named Entity Recognition visualization using spaCy
nlp = spacy.load('en_core_web_sm')
text = nlp('Hi, My name is datawithece \n I am from NL \n I want to work with FinTech to drive impact \n Beyonce is My Inspiration')
displacy.render(text, style='ent', jupyter=True)
